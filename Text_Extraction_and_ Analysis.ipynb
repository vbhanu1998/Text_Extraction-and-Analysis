{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f38a9e6",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "To extract textual data articles from the number of URL's and perfrom text analysis to compute following variables.\n",
    "\n",
    "1.\tPOSITIVE SCORE\n",
    "2.\tNEGATIVE SCORE\n",
    "3.\tPOLARITY SCORE\n",
    "4.\tSUBJECTIVITY SCORE\n",
    "5.\tAVG SENTENCE LENGTH\n",
    "6.\tPERCENTAGE OF COMPLEX WORDS\n",
    "7.\tFOG INDEX\n",
    "8.\tAVG NUMBER OF WORDS PER SENTENCE\n",
    "9.\tCOMPLEX WORD COUNT\n",
    "10.\tWORD COUNT\n",
    "11.\tSYLLABLE PER WORD\n",
    "12.\tPERSONAL PRONOUNS\n",
    "13.\tAVG WORD LENGTH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dec855",
   "metadata": {},
   "source": [
    "# STEP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "8e481d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: validators in c:\\users\\bhanu\\anaconda3\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\bhanu\\anaconda3\\lib\\site-packages (from validators) (5.1.0)\n",
      "Requirement already satisfied: syllables in c:\\users\\bhanu\\anaconda3\\lib\\site-packages (1.0.7)\n",
      "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in c:\\users\\bhanu\\anaconda3\\lib\\site-packages (from syllables) (5.2.0)\n",
      "Requirement already satisfied: cmudict<2.0.0,>=1.0.11 in c:\\users\\bhanu\\anaconda3\\lib\\site-packages (from syllables) (1.0.13)\n",
      "Requirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in c:\\users\\bhanu\\anaconda3\\lib\\site-packages (from cmudict<2.0.0,>=1.0.11->syllables) (5.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\bhanu\\anaconda3\\lib\\site-packages (from importlib-metadata<6.0.0,>=5.1.0->syllables) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install validators\n",
    "!pip install syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "6e83083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from urllib.request import urlopen\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load basic package and dataset for NLP\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import validators\n",
    "import requests\n",
    "import syllables\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Get the URL's\n",
    "input_file = pd.read_excel('Input.xlsx')\n",
    "input_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "63242a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We had 114 URL's contains two columns URL_ID and URL'S\n",
    "len(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd561048",
   "metadata": {},
   "source": [
    "# STEP 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cbf5b",
   "metadata": {},
   "source": [
    "# Checking for Invalid URLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "6c9ed7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "24ee00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "working_urls_id=[]\n",
    "not_working_urls_id=[]\n",
    "# try block to read URL\n",
    "for i in range(0,114):  \n",
    "    try:\n",
    "        html = urlopen(input_file['URL'].values[i])\n",
    "    except HTTPError as e:\n",
    "        #print(\"HTTP error\", e)\n",
    "        not_working_urls_id+=[i]\n",
    "\n",
    "    except URLError as e:\n",
    "        #print(\"Opps ! Page not found!\", e)\n",
    "        not_working_urls_id+=[i]\n",
    "\n",
    "    else:\n",
    "        #print('Yeah !  found ')\n",
    "        working_urls_id+=[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "9e66de50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 20, 107]"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_working_urls_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "eb79041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping URL's that are not working\n",
    "input_file.drop( input_file.index[not_working_urls_id], axis=0, inplace=True)\n",
    "input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "a16fb8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "c29f4837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = input_file['URL'].values\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "23a81df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping url's from output_file\n",
    "output_file = pd.read_excel('Output Data Structure.xlsx')\n",
    "output_file.drop(not_working_urls_id,axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "389dfd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387034c",
   "metadata": {},
   "source": [
    "# STEP 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fb8a2",
   "metadata": {},
   "source": [
    "# Merging stopword files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "d342c435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14238"
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given different files that contain stopwords cleaning and merging all those files\n",
    "import re\n",
    "sw_files = ['StopWords_Currencies.txt','StopWords_Geographic.txt','StopWords_Names.txt','StopWords_Auditor.txt','StopWords_DatesandNumbers.txt','StopWords_Generic.txt','StopWords_GenericLong.txt']\n",
    "user_stopwords =[]\n",
    "\n",
    "for name in sw_files:\n",
    "    with open(name) as sw:\n",
    "        sw_content = sw.read()\n",
    "        sw_content = sw_content.lower()\n",
    "        sw_content = sw_content.replace('|','')\n",
    "        sw_list = sw_content.split() \n",
    "        user_stopwords += [x for x in sw_list]\n",
    "\n",
    "len(user_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1d4c2",
   "metadata": {},
   "source": [
    "# Extracting Positive Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c58457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading positive words and cleaning\n",
    "\n",
    "import re\n",
    "positive_words = []\n",
    "with open('positive-words.txt') as pw:\n",
    "            pw_content = pw.read()\n",
    "            pw_content = pw_content.split('\\n')\n",
    "            positive_words = [x for x in pw_content]\n",
    "print(len(positive_words))\n",
    "print(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83799c87",
   "metadata": {},
   "source": [
    "# Extracting Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cddaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading negative words and cleaning\n",
    "\n",
    "import re\n",
    "negative_words = []\n",
    "with open('negative-words.txt') as nw:\n",
    "            nw_content = nw.read()\n",
    "            nw_content = nw_content.split('\\n')\n",
    "            negative_words = [x for x in nw_content]\n",
    "print(len(negative_words))\n",
    "print(negative_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281087e2",
   "metadata": {},
   "source": [
    "# STEP 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a79c56",
   "metadata": {},
   "source": [
    "# Extracting Data using Beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "967d30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to extract data, sentence tokenizaton, word tokenization. \n",
    "def Extracting_Data(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'lxml') \n",
    "    #title = soup.findAll('h1')[0].text\n",
    "    s= (soup.title.text).split()\n",
    "    title=' '.join(s[0:(len(s)-3)])\n",
    "    p_tag = soup.findAll('p', {'class' : ''})\n",
    "    String = ''\n",
    "    for i in p_tag:\n",
    "        N = i.text\n",
    "        String += ''.join(N)+ ' '\n",
    "    S = String\n",
    "    Data = title + \" \"+ String\n",
    "    Data = Data.lower()\n",
    "    return Data\n",
    "\n",
    "def sentence(Data):\n",
    "    sentences = nltk.sent_tokenize(Data)\n",
    "    corpus=[]\n",
    "    for i in range(len(sentences)):\n",
    "        review = re.sub('[^a-zA-Z]',' ', sentences[i] )\n",
    "        review = review.split()\n",
    "        r=[word for word in review if word not in user_stopwords]\n",
    "        r=' '.join(r)\n",
    "        corpus.append(r)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def total_words(sentence):\n",
    "    total_words=[]\n",
    "    for i in sentence:\n",
    "        total_words.extend(nltk.word_tokenize(i))\n",
    "        #total_words.append(words)        \n",
    "    return total_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66957d2",
   "metadata": {},
   "source": [
    "# STEP 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6473f06c",
   "metadata": {},
   "source": [
    "# Functions to Derrived variables\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df936098",
   "metadata": {},
   "source": [
    "1)positive_score \n",
    "2)negative_score\n",
    "3)polarity_score\n",
    "4)subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "0711467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def positive_score(words):\n",
    "    positive_score=0\n",
    "    for j in words:\n",
    "        if j in positive_words:\n",
    "            positive_score+=1\n",
    "    return positive_score\n",
    "\n",
    "            \n",
    "def negative_score(words):\n",
    "    negative_score=0\n",
    "    for j in words:\n",
    "        if j in negative_words:\n",
    "            negative_score+=1\n",
    "    return negative_score\n",
    "\n",
    "def polarity_score(positive_score,negative_score):\n",
    "    polarity_score = (positive_score - negative_score)/ ((positive_score + negative_score) + 0.000001)\n",
    "    return polarity_score\n",
    "\n",
    "\n",
    "def subjectivity_score(positive_score,negative_score,total_words):\n",
    "    subjectivity_score = (positive_score + negative_score)/ ((len(total_words)) + 0.000001)\n",
    "    return subjectivity_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622f9b23",
   "metadata": {},
   "source": [
    "# Functions for Analysis of Readability"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb3dedad",
   "metadata": {},
   "source": [
    "1)Average Sentence Length\n",
    "2)Percentage of Complex words\n",
    "3)Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "f648a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_lengths(l_w,l_s):\n",
    "    #Average_length_words = (len(total_words))/(len(sentence))\n",
    "    Average_length_words = (((l_w))/((l_s)))\n",
    "    return float(Average_length_words)\n",
    "\n",
    "\n",
    "def per_of_complex_numbers(total_words):\n",
    "    complex_words=[]\n",
    "    complex_count=0\n",
    "    for j in total_words:\n",
    "        if (syllables.estimate(j))>2:\n",
    "            complex_words.append(j)\n",
    "            complex_count+=1\n",
    "    percentage = ((complex_count)/(len(total_words)))*100\n",
    "    return percentage\n",
    "\n",
    "def fog_index(Average_length_words, per_complex_words):\n",
    "    Fog_Index = 0.4 * (Average_length_words + per_complex_words)\n",
    "    return float(Fog_Index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fc4b1ec",
   "metadata": {},
   "source": [
    "Fuctions to get:\n",
    "    1)Average Number of Words Per Sentence\n",
    "    2)Complex Word Count\n",
    "    3)Word Count\n",
    "    4)Personal Pronouns\n",
    "    5)Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "4ccf3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_no_words_per_sec(l_w,l_s):\n",
    "    #Average_Number_of_Words_Per_Sentence = len(total_words) / len(corpus)\n",
    "    Average_Number_of_Words_Per_Sentence = (l_w)/(l_s)\n",
    "    return Average_Number_of_Words_Per_Sentence\n",
    "    \n",
    "def complex_words_count(total_words):\n",
    "    complex_words=[]\n",
    "    complex_count=0\n",
    "    for j in total_words:\n",
    "        if (syllables.estimate(j))>2:\n",
    "            complex_words.append(j)\n",
    "            complex_count+=1\n",
    "    return complex_count\n",
    "\n",
    "def word_count(data):   \n",
    "    new_text=[]\n",
    "    for i in data:\n",
    "        x=i.split()\n",
    "        for j in range(0,len(x)):\n",
    "            if x[j] in stopwords.words('english'):\n",
    "                new_text.append('')\n",
    "            else:\n",
    "                new_text.append(j)          \n",
    "    return len(new_text)\n",
    "\n",
    "\n",
    "\n",
    "def personal_pronouns(text):\n",
    "    pronouns = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "    pronoun = pronouns.findall(text)\n",
    "    return len(pronoun)\n",
    "\n",
    "def avg_word_length(word):\n",
    "    s=0\n",
    "    for i in word:\n",
    "        s+=len(i)\n",
    "    Avg_w_L = (s)/(len(word))\n",
    "    return Avg_w_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58357dfe",
   "metadata": {},
   "source": [
    "# STEP 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93102d17",
   "metadata": {},
   "source": [
    "# Calculating all scores and appending in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "id": "6db00e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_score=[]\n",
    "n_score=[]\n",
    "pol_score=[]\n",
    "sub=[]\n",
    "avg_sentence_length = []\n",
    "per_complex_w = []\n",
    "for_index = []\n",
    "avg_number_words_per_second=[]\n",
    "com_count=[]\n",
    "word_counts=[]\n",
    "personal_pronoun=[]\n",
    "avg_w_len =[]\n",
    "\n",
    "\n",
    "#For loop takes each url and calculate all scores and append in lists\n",
    "for i in urls:\n",
    "    d=Extracting_Data(i)\n",
    "    s=sentence(d)\n",
    "    t_w = total_words(s)\n",
    "    length_sentence = len(s)\n",
    "    length_word = len(t_w)\n",
    "    p=positive_score(t_w)\n",
    "    n=negative_score(t_w)\n",
    "    pol=polarity_score(p,n)\n",
    "    sub_score = subjectivity_score(p,n,t_w)\n",
    "    avg_sent_length = avg_sentence_lengths(length_word,length_sentence)\n",
    "    per_c_w =per_of_complex_numbers(t_w)\n",
    "    fog_ind = fog_index(avg_sent_length,per_c_w)\n",
    "    avg_number_words_per_sec = avg_no_words_per_sec(length_word,length_sentence)\n",
    "    complex_count = complex_words_count(t_w)\n",
    "    w_c = word_count(d)\n",
    "    p_pronoun =personal_pronouns(d)\n",
    "    avg_word_len =avg_word_length(t_w)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Appending all values into above lists\n",
    "    p_score.append(p)\n",
    "    n_score.append(n)\n",
    "    pol_score.append(pol)\n",
    "    sub.append(sub_score)\n",
    "    avg_sentence_length.append(avg_sent_length)\n",
    "    per_complex_w.append(per_c_w)\n",
    "    for_index.append(fog_ind)\n",
    "    avg_number_words_per_second.append(avg_number_words_per_sec)\n",
    "    word_counts.append(w_c)\n",
    "    com_count.append(complex_count)\n",
    "    personal_pronoun.append(p_pronoun)\n",
    "    avg_w_len.append(avg_word_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7cf02",
   "metadata": {},
   "source": [
    "# Created DataFrame and stored all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "b9bcf9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "35ec6c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_file['URL_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "id": "1f125ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['URL_ID']=input_file['URL_ID']\n",
    "df['POSITIVE SCORE']= p_score\n",
    "df['NEGATIVE SCORE']=n_score\n",
    "df['POLARITY SCORE']=pol_score \n",
    "df['SUBJECTIVITY SCORE'] = sub\n",
    "df['AVG SENTENCE LENGTH'] = avg_sentence_length\n",
    "df['PERCENTAGE OF COMPLEX WORDS'] = per_complex_w\n",
    "df['FOG INDEX'] =  for_index \n",
    "df['AVG NUMBER OF WORDS PER SENTENCE'] = avg_number_words_per_second\n",
    "df['COMPLEX WORD COUNT'] =com_count\n",
    "df['WORD COUNT'] =  word_counts\n",
    "df['PERSONAL PRONOUNS'] = personal_pronoun\n",
    "df['AVG WORD LENGTH'] = avg_w_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "8c0f348e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 13)"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "id": "cf37e4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>66</td>\n",
       "      <td>34</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.104493</td>\n",
       "      <td>12.760000</td>\n",
       "      <td>51.515152</td>\n",
       "      <td>25.710061</td>\n",
       "      <td>12.760000</td>\n",
       "      <td>493</td>\n",
       "      <td>10316</td>\n",
       "      <td>1</td>\n",
       "      <td>7.579937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>58</td>\n",
       "      <td>37</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.176580</td>\n",
       "      <td>6.810127</td>\n",
       "      <td>42.379182</td>\n",
       "      <td>19.675723</td>\n",
       "      <td>6.810127</td>\n",
       "      <td>228</td>\n",
       "      <td>7037</td>\n",
       "      <td>7</td>\n",
       "      <td>7.154275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.0</td>\n",
       "      <td>64</td>\n",
       "      <td>35</td>\n",
       "      <td>0.292929</td>\n",
       "      <td>0.123288</td>\n",
       "      <td>9.447059</td>\n",
       "      <td>55.292653</td>\n",
       "      <td>25.895885</td>\n",
       "      <td>9.447059</td>\n",
       "      <td>444</td>\n",
       "      <td>9352</td>\n",
       "      <td>3</td>\n",
       "      <td>7.712329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>60</td>\n",
       "      <td>27</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.147708</td>\n",
       "      <td>6.402174</td>\n",
       "      <td>43.803056</td>\n",
       "      <td>20.082092</td>\n",
       "      <td>6.402174</td>\n",
       "      <td>258</td>\n",
       "      <td>7846</td>\n",
       "      <td>18</td>\n",
       "      <td>7.185059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>0.116246</td>\n",
       "      <td>9.272727</td>\n",
       "      <td>47.338936</td>\n",
       "      <td>22.644665</td>\n",
       "      <td>9.272727</td>\n",
       "      <td>338</td>\n",
       "      <td>8850</td>\n",
       "      <td>18</td>\n",
       "      <td>7.327731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID  POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0    37.0              66              34        0.320000            0.104493   \n",
       "1    38.0              58              37        0.221053            0.176580   \n",
       "2    39.0              64              35        0.292929            0.123288   \n",
       "3    40.0              60              27        0.379310            0.147708   \n",
       "4    41.0              58              25        0.397590            0.116246   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0            12.760000                    51.515152  25.710061   \n",
       "1             6.810127                    42.379182  19.675723   \n",
       "2             9.447059                    55.292653  25.895885   \n",
       "3             6.402174                    43.803056  20.082092   \n",
       "4             9.272727                    47.338936  22.644665   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                         12.760000                 493       10316   \n",
       "1                          6.810127                 228        7037   \n",
       "2                          9.447059                 444        9352   \n",
       "3                          6.402174                 258        7846   \n",
       "4                          9.272727                 338        8850   \n",
       "\n",
       "   PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                  1         7.579937  \n",
       "1                  7         7.154275  \n",
       "2                  3         7.712329  \n",
       "3                 18         7.185059  \n",
       "4                 18         7.327731  "
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efb3e23",
   "metadata": {},
   "source": [
    "# STEP 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74b485",
   "metadata": {},
   "source": [
    "# Writing into output excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "c4739e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlwt in c:\\users\\bhanu\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xlwt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "d00375d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\bhanu\\anaconda3\\lib\\site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\bhanu\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "afe25f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
    "           'PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE',\n",
    "           'COMPLEX WORD COUNT','WORD COUNT','PERSONAL PRONOUNS','AVG WORD LENGTH']\n",
    "for i in output_file:\n",
    "    for j in columns:\n",
    "        output_file[j]=df[j]\n",
    "        \n",
    "output_file.to_excel('output_Data_Final.xlsx', sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa77aa",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ba204",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Syllable per word was not filled in result output sheet as it contains list of syllable words\n",
    "2. Final output data is in output_Data_Final.xlsx file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
